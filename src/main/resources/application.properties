quarkus.langchain4j.ollama.chat-model.model-id=tinyllama
# In MacOS the DevServices with Ollama is not working and we need to have Ollama running in the background
quarkus.langchain4j.ollama.devservices.model=tinyllama
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.timeout=60s

quarkus.fx.fxml-root=/fxml/
